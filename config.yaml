experiment:
  name: "Cross-Lingual Dry-Run"
  version: "v2.x.0"
  dir: "./experiments/v2.x/v2.x.0"
  baseline: "v2.x"
  description: "Use modified TNCSE approach in cross-lingual mode on English subtask2 with AEDA data augmentation on minority_classes"
  author: Emmanuel Olateju
  date: 01-02-2026
  messages:
    - Second dr Run
    - v2.x already gave 0.24 F1 macro on English cross-lingual subtask-2 greated than the POLAR papear 0.1969
    - Now I am seraching augmentation methods that will bring it to above mono-lingual 0.3138, since my cross-lingual uses part of the target language in the train split. 

languages:
  - "swa"
  - "hau"
  - "eng"
  - "amh"

models:
  slave_model: "setu4993/LEALLA-base"
  anchor_model: "sentence-transformers/LaBSE"
  freeze_slave_n_layers: 0
  freeze_anchor_n_layers: 0

training:
  lr: !!float 2e-5
  batch_size: 16  
  n_epochs: 10
  
  TNCSE:
    alpha: 0.05
    beta: 1
    gamma: 0.01
    temperature: 0.1

n_labels:
  subtask1: 2
  subtask2: 5
  subtask3: 7

minority_classes:
  subtask2:
   - "political"

augmentations:
  - "aeda"