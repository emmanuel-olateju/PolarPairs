experiment:
  name: "Language Decoupled (Independent / Mono-Lingual) Performance"
  version: "v1.0.3"
  dir: "./experiments/v1.0/v1.0.3"
  baseline: "v1.0"
  description: "Train different model instance for each language using train-val-test split of 70-10-20. Goal is to identify what model consistently performs best across languages and subtasks "
  author: Emmanuel Olateju
  date: 31-01-2026
  messages:
    - "Switched to sentence-transformers/LaBSE"
    - "Here we want to test full fine-tuning of a large model to see if previous smaller/distilled models are as good as it"

languages:
  - "swa"
  - "hau"
  - "eng"
  - "amh"

models:
  slave_model: "sentence-transformers/LaBSE"
  anchor_model: None

training:
  lr: !!float 1e-4
  batch_size: 8
  n_epochs: 5

n_labels:
  subtask1: 2
  subtask2: 5
  subtask3: 7