experiment:
  name: "Cross-Lingual Dry-Run"
  version: "v2.x.1"
  dir: "./experiments/v2.x/v2.x.1"
  baseline: "v2.x.0"
  description: "Use modified TNCSE approach in cross-lingual mode on English subtask2 with AEDA + wordswap + back-translate data augmentation on minority_classes"
  author: Emmanuel Olateju
  date: 01-02-2026
  messages:
    - Third dry run
    - v2.x.0 already gave 0.35 F1 macro on English cross-lingual subtask-2 greated than the POLAR papear 0.1969
    - Now I am adding back-translation augmentation if it ill bring it to above mono-lingual 0.35 of v2.x.0; since my cross-lingual uses part of the target language in the train split I need the best eval_f1_macto possible without any overfitting risk.

languages:
  - "swa"
  - "hau"
  - "eng"
  - "amh"

models:
  slave_model: "setu4993/LEALLA-base"
  anchor_model: "sentence-transformers/LaBSE"
  freeze_slave_n_layers: 0
  freeze_anchor_n_layers: 0

training:
  lr: !!float 2e-5
  batch_size: 16  
  n_epochs: 10
  
  TNCSE:
    alpha: 0.05
    beta: 1
    gamma: 0.01
    temperature: 0.1

n_labels:
  subtask1: 2
  subtask2: 5
  subtask3: 6

minority_classes:
  subtask2:
   - "gender/sexual"
   - "other"

augmentations:
  - "aeda"
  - "wordswap"
  - "backtranslate"