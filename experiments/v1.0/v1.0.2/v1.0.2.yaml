author: Emmanuel Olateju
baseline: v1.0
date: 31-01-2026
description: 'Train different model instance for each language using train-val-test
  split of 70-10-20. Goal is to identify what model consistently performs best across
  languages and subtasks '
messages:
- Switched to cardiffnlp/twitter-roberta-base-sentiment-latest
- Here we want to test full fine-tuning of a large model to see if previous smaller/distilled
  models are as good as it
name: Language Decoupled (Independent / Mono-Lingual) Performance
parameters:
  Performance:
    amh_eval_results:
      epoch: 5.0
      eval_f1_macro: 0.44269887770259114
      eval_loss: 0.5432895421981812
      eval_runtime: 1.5547
      eval_samples_per_second: 429.014
      eval_steps_per_second: 54.029
    eng_eval_results:
      epoch: 5.0
      eval_f1_macro: 0.8190051020408163
      eval_loss: 0.6997514367103577
      eval_runtime: 1.168
      eval_samples_per_second: 552.23
      eval_steps_per_second: 69.35
    hau_eval_results:
      epoch: 5.0
      eval_f1_macro: 0.8274703290775041
      eval_loss: 0.20617999136447906
      eval_runtime: 1.5828
      eval_samples_per_second: 461.848
      eval_steps_per_second: 58.126
    swa_eval_results:
      epoch: 5.0
      eval_f1_macro: 0.7811908373848437
      eval_loss: 0.6322900056838989
      eval_runtime: 2.9767
      eval_samples_per_second: 469.989
      eval_steps_per_second: 58.791
  Training:
    anchor_model: None
    batch_size: 8
    lr: 0.0001
    n_epochs: 5
    slave_model: cardiffnlp/twitter-roberta-base-sentiment-latest
version: v1.0.2
