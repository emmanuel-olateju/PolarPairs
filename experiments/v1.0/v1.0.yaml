author: Emmanuel Olateju
baseline: v1.0
date: 31-01-2026
description: 'Train different model instance for each language using train-val-test
  split of 70-10-20. Goal is to identify what model consistently performs best across
  languages and subtasks '
messages:
- Start with mDeBERTa-small
name: Language Decoupled (Independent / Mono-Lingual) Performance
parameters:
  Performance:
    amh_eval_results:
      epoch: 5.0
      eval_f1_macro: 0.430401366353544
      eval_loss: 0.5163150429725647
      eval_runtime: 1.747
      eval_samples_per_second: 381.799
      eval_steps_per_second: 48.083
    eng_eval_results:
      epoch: 5.0
      eval_f1_macro: 0.8003820541278921
      eval_loss: 0.6631384491920471
      eval_runtime: 1.6552
      eval_samples_per_second: 389.683
      eval_steps_per_second: 48.937
    hau_eval_results:
      epoch: 5.0
      eval_f1_macro: 0.8284037558685446
      eval_loss: 0.1898168921470642
      eval_runtime: 1.7063
      eval_samples_per_second: 428.405
      eval_steps_per_second: 53.917
    swa_eval_results:
      epoch: 5.0
      eval_f1_macro: 0.764750769859828
      eval_loss: 0.6086798310279846
      eval_runtime: 4.5212
      eval_samples_per_second: 309.432
      eval_steps_per_second: 38.707
  Training:
    anchor_model: None
    batch_size: 8
    lr: 0.0001
    n_epochs: 5
    slave_model: microsoft/deberta-v3-small
version: v1.0
